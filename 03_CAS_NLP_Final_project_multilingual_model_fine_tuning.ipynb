{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBaffelli/CAS-NLP_Machine-translation/blob/main/03_CAS_NLP_Final_project_multilingual_model_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eGp9jCvw2Yhq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate sacrebleu accelerate -U bert_score rouge_score peft sacremoses torch pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IriPpH2YtItc"
      },
      "source": [
        "# **Fine-tuning**\n",
        "\n",
        "This script is used for the fine-tuning of a machine translation model from EN into 5 languages (ES, FR, IT, PT, RO). Supports T5 models (T5 and flanT5) as well as OPUS models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKvh2T7uiv8J"
      },
      "outputs": [],
      "source": [
        "#@title Imports and varia\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GCH0YEz3HpN"
      },
      "outputs": [],
      "source": [
        "#@title Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDvJBicJCVY_"
      },
      "outputs": [],
      "source": [
        "#@title Load data\n",
        "#Load the datasets\n",
        "italian = pd.read_csv('/content/drive/MyDrive/CAS NLP/Final_project/Dataset/data_for_training/italian.csv', dtype=str)\n",
        "french = pd.read_csv('/content/drive/MyDrive/CAS NLP/Final_project/Dataset/data_for_training/french.csv', dtype=str)\n",
        "spanish = pd.read_csv('/content/drive/MyDrive/CAS NLP/Final_project/Dataset/data_for_training/spanish.csv', dtype=str)\n",
        "romanian = pd.read_csv('/content/drive/MyDrive/CAS NLP/Final_project/Dataset/data_for_training/romanian.csv', dtype=str)\n",
        "portuguese = pd.read_csv('/content/drive/MyDrive/CAS NLP/Final_project/Dataset/data_for_training/portuguese.csv', dtype=str)\n",
        "\n",
        "#Create a mapping to iterate in the dataframes\n",
        "languages = {\n",
        "    'Italian': italian,\n",
        "    'French': french,\n",
        "    'Spanish' : spanish,\n",
        "    'Romanian' : romanian,\n",
        "    'Portuguese' : portuguese\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TRHXSX_DJXaR"
      },
      "outputs": [],
      "source": [
        "#@title Load tokenizer and model\n",
        "#models: google-t5/t5-small, google-t5/t5-base, google-t5/t5-large, google/flan-t5-small, google/flan-t5-base, google/flan-t5-large, Helsinki-NLP/opus-mt-en-roa\n",
        "#Get the tokenizer and the model\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-en-roa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUEH-KXXjp7u"
      },
      "outputs": [],
      "source": [
        "#@title Pre-process dataset and append model-specific prefix\n",
        "#Creates a mapping for the prefixes\n",
        "prefix_mapping_T5 = {\n",
        "    'Italian' : 'translate English to Italian: ',\n",
        "    'French' : 'translate English to French: ',\n",
        "    'Spanish' : 'translate English to Spanish: ',\n",
        "    'Romanian' : 'translate English to Romanian: ',\n",
        "    'Portuguese' : 'translate English to Portuguese: '\n",
        "}\n",
        "\n",
        "prefix_mapping_OPUS =  {\n",
        "    'Italian' : '>>ita<< ',\n",
        "    'French' : '>>fra<< ',\n",
        "    'Spanish' : '>>spa<< ',\n",
        "    'Romanian' : '>>ron<< ',\n",
        "    'Portuguese' : '>>por<< '\n",
        "}\n",
        "\n",
        "#Function to transform the dataset and append the correct prefix\n",
        "##Returns the dataset in the Hugginface dataset format\n",
        "def transform_dataset_append_prefix(name, df):\n",
        "  prefix = ''\n",
        "  dataset = []\n",
        "  if 'opus' in checkpoint:\n",
        "    prefix = prefix_mapping_OPUS[name]\n",
        "  elif 't5' in checkpoint:\n",
        "    prefix = prefix_mapping_T5[name]\n",
        "  for index, row in df.iterrows():\n",
        "    translation = {'en': prefix + row['sourceExpression'], 'target': row['targetExpression']}\n",
        "    data = {'translation': translation}\n",
        "    dataset.append(data)\n",
        "  return Dataset.from_pandas(pd.DataFrame(data=dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWTyZJO-n_q-"
      },
      "outputs": [],
      "source": [
        "#@title Apply tokenizer\n",
        "#Function to preprocess and tokenize the data\n",
        "##We need to preprocess and prepare the data for the fine-tuning\n",
        "max_length = 128\n",
        "source_lang = \"en\"\n",
        "def preprocess_function(examples):\n",
        "  inputs = [example[source_lang] for example in examples[\"translation\"]]\n",
        "  targets = [example['target'] for example in examples[\"translation\"]]\n",
        "  model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
        "  return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ28jWhMkqUj"
      },
      "outputs": [],
      "source": [
        "#Transform the dataset in the Huggingface dataset format, apply the tokenization\n",
        "datasets = {}\n",
        "for language_name, language_df in languages.items():\n",
        "  initial_dataset = transform_dataset_append_prefix(language_name, language_df)\n",
        "  datasets[language_name] = initial_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je-uRA_wppbC"
      },
      "outputs": [],
      "source": [
        "#Once we have prepared all the datasets, we can combine them together in a single dataset\n",
        "combined_dataset = concatenate_datasets(list(datasets.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXUrMIvUN8Nz"
      },
      "outputs": [],
      "source": [
        "#@title Split the data into train and test\n",
        "#Then we split the data into train and test set\n",
        "#Split 20% for testing\n",
        "train_test_split = combined_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "#Create a DatasetDict to hold the splits\n",
        "final_dataset = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'test': train_test_split['test']\n",
        "})\n",
        "\n",
        "#We create a data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tat1RP-36U8"
      },
      "outputs": [],
      "source": [
        "#@title Import what is needed for the evaluation\n",
        "#We import the score for the evaluation\n",
        "sacrebleu_score = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTnIFZ9v372A"
      },
      "outputs": [],
      "source": [
        "#@title Functions needed to compute metrics\n",
        "#Function to post-process text\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "#Function to compute the metrics\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = sacrebleu_score.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "    )\n",
        "    result = {\"sacrebleu\":  round(result[\"score\"], 4)}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "709dILmQx_qA"
      },
      "outputs": [],
      "source": [
        "#@title Freeze the embedding layers\n",
        "for name, param in model.named_parameters():\n",
        "    if 'shared' in name:  # 'shared' is commonly used for the embedding parameter\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h73h4-BDv9T9"
      },
      "outputs": [],
      "source": [
        "#@title Load the PEFT configuration\n",
        "#Load the model with the PEFT config\n",
        "peft_config = ''\n",
        "if 't5' in checkpoint:\n",
        "  peft_config = LoraConfig(\n",
        "      task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
        "  )\n",
        "else:\n",
        "    peft_config = LoraConfig(\n",
        "      task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
        "      target_modules = ['k_proj', 'v_proj', 'q_proj', 'out_proj']\n",
        "  )\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl3b_2Ad4BL3"
      },
      "outputs": [],
      "source": [
        "#@title Define hyperparameters, training arguments, and the sequence2sequence trainer\n",
        "#Hyperparameters and misc\n",
        "learning_rate = 1e-03\n",
        "batch_size = 32\n",
        "epochs = 4\n",
        "\n",
        "#We define the training arguments and the trainer\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=epochs,\n",
        "    predict_with_generate=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    dataloader_num_workers=4,\n",
        "    fp16=True,\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_dataset[\"train\"],\n",
        "    eval_dataset=final_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJckIRqgA4_B"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v4EFWW8U4Di1"
      },
      "outputs": [],
      "source": [
        "#@title Start the fine-tuning process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvt2gn8oCodX"
      },
      "outputs": [],
      "source": [
        "#@title Save the model\n",
        "trainer.save_model('')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO1VhgvLl56O2BOR24njUwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}