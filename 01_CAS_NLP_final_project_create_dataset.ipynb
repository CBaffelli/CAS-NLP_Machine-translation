{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP93XHq1CGssRZqwwc7gCTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CBaffelli/CAS-NLP_Machine-translation/blob/main/01_CAS_NLP_final_project_create_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset pre-processing**\n",
        "\n",
        "This script is used to create an initial dataset from the original corpus in JSON format."
      ],
      "metadata": {
        "id": "NlkgHeAqSQwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and varia\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "6znytITnqZ67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llx_fqKnouT-",
        "outputId": "234eebf2-b220-4d5b-b7dd-74dc75d16038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "path_to_data = ''"
      ],
      "metadata": {
        "id": "U5xrBEqGqRss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data is provided in several JSON files, we iterate through all of them, we group the data when possible, and we create a combined dataframe."
      ],
      "metadata": {
        "id": "g68GzYWQbMVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to process each JSON file\n",
        "def process_json_file(filename):\n",
        "    df = pd.read_json(os.path.join(path_to_data, filename), encoding='utf-8')\n",
        "    columns_to_drop = ['translationId', 'createdAt', 'fileType', 'origin', 'translationVendor', 'reviewStatus']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "    grouped = df.groupby(['sourceExpression', 'sourceLanguage', 'targetExpression', 'targetLanguage'])['pimCode'].unique().apply(', '.join).reset_index()\n",
        "    return grouped"
      ],
      "metadata": {
        "id": "wtXCwv4uDwt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterate through JSON files and create the dataframe\n",
        "dataframes = [process_json_file(filename) for filename in os.listdir(path_to_data) if filename.endswith('.json')]\n",
        "#Concatenate all the dataframes\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "#Reset the index of the modified dataframe\n",
        "combined_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "_QqppfWED0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have a combined dataframe, we can do some cleanup on it:\n",
        "\n",
        "\n",
        "*   Harmonize the language codes to a standard mapping;\n",
        "*   Ensure that \"en-US\" is always the source language, and if needed switch the data accordingly;\n",
        "* Replace special characters with the correct encoding;\n",
        "* Remove all markup tags (from XML);\n",
        "* Remove unneeded spaces;\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60U6qCxrbVc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Harmonize all language codes using a mapping\n",
        "language_mapping = {\n",
        "    'en': 'en-US',\n",
        "    'de': 'de-DE',\n",
        "    'da': 'da-DK',\n",
        "    'cs': 'cs-CZ',\n",
        "    'el': 'el-GR',\n",
        "    'en-GB': 'en-US',\n",
        "    'bg-bg': 'bg-BG',\n",
        "    'de-de': 'de-DE',\n",
        "    'fr': 'fr-FR',\n",
        "    'vi': 'vi-VN',\n",
        "    'it': 'it-IT',\n",
        "    'ja': 'ja-JP',\n",
        "    'pt': 'pt-PT',\n",
        "    'ru': 'ru-RU',\n",
        "    'sv': 'sv-SE',\n",
        "    'no': 'nb-NO',\n",
        "    'pl': 'pl-PL',\n",
        "    'es': 'es-ES',\n",
        "    'zh-Ha': 'zh-CN',\n",
        "    'sk': 'sk-SK',\n",
        "    'ro': 'ro-RO',\n",
        "    'hu': 'hu-HU',\n",
        "    'nb': 'nb-NO',\n",
        "    'nn-NO': 'nb-NO',\n",
        "    'lv': 'lv-LV',\n",
        "    'fi': 'fi-FI',\n",
        "    'et': 'et-EE',\n",
        "    'zh': 'zh-CN',\n",
        "    'lt': 'lt-LT',\n",
        "    'ko': 'ko-KR',\n",
        "    'sr': 'sr-RS',\n",
        "    'es-x-int-SDL': 'es-ES',\n",
        "    'es-419': 'es-ES',\n",
        "    'sr-Latn-RS': 'sr-RS',\n",
        "    'nl': 'nl-NL',\n",
        "    'tr': 'tr-TR',\n",
        "    'vi-VI' : 'vi-VN',\n",
        "    'bg' : 'bg-BG'\n",
        "}\n",
        "\n",
        "combined_df['sourceLanguage'] = combined_df['sourceLanguage'].replace(language_mapping)\n",
        "combined_df['targetLanguage'] = combined_df['targetLanguage'].replace(language_mapping)"
      ],
      "metadata": {
        "id": "U4AgODHWcJQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter the DataFrame based on condition that the sourceLanguage shall always be en-US. If otherwise, swap the languages.\n",
        "filtered_df = combined_df[(combined_df['sourceLanguage'] == 'en-US') | ((combined_df['sourceLanguage'] != 'en-US') & (combined_df['targetLanguage'] == 'en-US'))]\n",
        "filtered_df.loc[filtered_df['sourceLanguage'] != 'en-US', ['sourceExpression', 'targetExpression']] = filtered_df.loc[filtered_df['sourceLanguage'] != 'en-US', ['targetExpression', 'sourceExpression']].values\n",
        "#Convert pimCode column to string\n",
        "filtered_df['pimCode'] = filtered_df['pimCode'].astype(str)"
      ],
      "metadata": {
        "id": "k0HL3-bfdKfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace some special characters\n",
        "character_mapping = {\n",
        "    '&amp;': '',\n",
        "    '&lt;': '<',\n",
        "    '&gt;': '>',\n",
        "    '&quot;': '\"',\n",
        "    'Non Breaking Hyphen Tag Text' : ' ',\n",
        "    '&apos;' : \"'\"\n",
        "}\n",
        "\n",
        "filtered_df['sourceExpression'] = filtered_df['sourceExpression'].replace(character_mapping, regex=True)\n",
        "filtered_df['targetExpression'] = filtered_df['targetExpression'].replace(character_mapping, regex=True)"
      ],
      "metadata": {
        "id": "f1PzaFhkd9us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove XML tags and markups\n",
        "def remove_xml_tags(text):\n",
        "    if isinstance(text, str):\n",
        "        cleaned_text = re.sub(r'<[^>]+>', '', text)\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "#Apply the function to the text column\n",
        "filtered_df['sourceExpression'] = filtered_df['sourceExpression'].apply(remove_xml_tags)\n",
        "filtered_df['targetExpression'] = filtered_df['targetExpression'].apply(remove_xml_tags)"
      ],
      "metadata": {
        "id": "kiurk3KUhbmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newlines, tabs, and other spaces\n",
        "filtered_df['sourceExpression'] = filtered_df['sourceExpression'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "filtered_df['targetExpression'] = filtered_df['targetExpression'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ],
      "metadata": {
        "id": "_OwSV2fi8bAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Strip leading and trailing spaces in the dataframe\n",
        "filtered_df = filtered_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "ZM-4hPe-Gs5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter the dataframe to keep only non-empty values in both columns\n",
        "filtered_df = filtered_df[(filtered_df['sourceExpression'] != '') & (filtered_df['targetExpression'] != '')]"
      ],
      "metadata": {
        "id": "c1ZRSaN09mBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a regular expression pattern to match sentences with only special characters or numbers\n",
        "pattern = r'^[\\W\\d]+$'\n",
        "\n",
        "#Apply the pattern to both sourceExpression and targetExpression columns\n",
        "mask = filtered_df['sourceExpression'].str.match(pattern) | filtered_df['targetExpression'].str.match(pattern)\n",
        "\n",
        "#Filter out the rows that match the pattern\n",
        "filtered_df = filtered_df[~mask]"
      ],
      "metadata": {
        "id": "LsARMBkXxr5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate the data again\n",
        "#Convert all to string\n",
        "filtered_df = filtered_df.astype(str)\n",
        "aggregated_data = filtered_df.groupby(['sourceExpression', 'sourceLanguage', 'targetExpression', 'targetLanguage'])['pimCode'].unique().apply(', '.join).reset_index()"
      ],
      "metadata": {
        "id": "1D8noshTcuyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if a sentence is more than 50% non-alphanumeric\n",
        "def is_valid_sentence(row):\n",
        "    source_sentence = row['sourceExpression']\n",
        "    target_sentence = row['targetExpression']\n",
        "\n",
        "    total_chars_source = len(source_sentence)\n",
        "    non_alpha_chars_source = sum([1 for char in source_sentence if char not in string.ascii_letters])\n",
        "    source_valid = non_alpha_chars_source / total_chars_source <= 0.4\n",
        "\n",
        "    total_chars_target = len(target_sentence)\n",
        "    non_alpha_chars_target = sum([1 for char in target_sentence if char not in string.ascii_letters])\n",
        "    target_valid = non_alpha_chars_target / total_chars_target <= 0.4\n",
        "\n",
        "    return source_valid and target_valid\n",
        "\n",
        "def clean_dataset(df):\n",
        "    cleaned_df = df[df.apply(is_valid_sentence, axis=1)]\n",
        "    return cleaned_df"
      ],
      "metadata": {
        "id": "jzXzRT-yoRPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_data = clean_dataset(aggregated_data)"
      ],
      "metadata": {
        "id": "8ktrcaseoWKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the full dataset"
      ],
      "metadata": {
        "id": "9Q7MfXX0OTWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_data.to_csv('MT_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "qeWxFdK75CLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each language, we create a sub-dataset"
      ],
      "metadata": {
        "id": "Jz9H92tWSwLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create sub-datasets for each language pair\n",
        "groups = aggregated_data.groupby(['sourceLanguage', 'targetLanguage'])\n",
        "sub_datasets = {}\n",
        "for (source_lang, target_lang), group in groups:\n",
        "    #Filter out sourceExpressions and targetExpressions based on sentence length\n",
        "    group['Source length'] = group['sourceExpression'].str.split().str.len()\n",
        "    group['Target length'] = group['targetExpression'].str.split().str.len()\n",
        "    group = group[group['Source length'] <80]\n",
        "    group = group[group['Target length'] <80]\n",
        "    group = group[group['Target length'] >5]\n",
        "    group = group[group['Source length'] >5]\n",
        "    # Remove the columns sourceLanguage and targetLanguage as they are not needed\n",
        "    group = group.drop(['sourceLanguage', 'targetLanguage', 'Source length', 'Target length'], axis=1)\n",
        "\n",
        "    # Add the filtered group to the sub-datasets dictionary\n",
        "    sub_datasets[f'{source_lang}_{target_lang}'] = group\n",
        "\n",
        "#In each sub-dataset, remove duplicates\n",
        "#Save each sub-dataset to a CSV file\n",
        "for language_pair, sub_dataset in sub_datasets.items():\n",
        "    #Drop duplicates based on 'sourceExpression'\n",
        "    sub_dataset.drop_duplicates(subset=['sourceExpression'], keep='first', inplace=True)\n",
        "    #Drop duplicates based on 'targetExpression'\n",
        "    sub_dataset.drop_duplicates(subset=['targetExpression'], keep='first', inplace=True)\n",
        "    sub_dataset[['sourceExpression', 'targetExpression', 'pimCode']].to_csv(f'{language_pair}.csv', index=False)\n"
      ],
      "metadata": {
        "id": "H74cb2aZyNYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}